# misinformation
Sentiment Analysis ML model to determine whether or not an article contains misinformation based off of an article's headline.

In the 2016 election, the United States electorate faced misinformation on a scale never previously seen before. Seemingly reputable articles contained either misinformation or outright lies, leading to distrust among others. Given the large amounts of articles that were produced, it becomes important to algorithmically flag which articles could be potentially dangerous. My model attempts uses Natual Langauge Processing in order to classify the articles based on the titles of the articles. I decided to analyze the titles of the articles instead of the articles themselves because that method saves on memory and computation time.

newsfinal.xlsx contains a list of news article titles which are marked as either with either a 1 or a 0. A 1 indicates that the original article was real, while a 0 indicates that the original article was either fake or misleading. I obtained the list of articles from an online database containing news articles from the time. Then, I loaded the dataset into misinfo.py. There I tokenized the words from the corpus and made the max length of the titles 50 words with a max subword length of 5. I also truncated and padded the titles at the end because I assumed that the most important information of a title would be at its beginning. Next, I split the data into training and validation sets, and tested it over several different machine learning models.

The machine learning model that provided the most accuracy consisted of two bidirectional LSTMs followed by one dense layer with a relu activation function, and then the output layer with a sigmoid activation function. I used LSTMs because they would allow the model to identify both long term patterns with the short term memory. I eventually determined that training the set for 8 epochs provided the best accuracy for the model, with an accuracy of ~93% for the training data and ~81% for validation data. Although more epochs resulted in an accuracy of ~97% for the training data, it came with a validation accuracy of ~78%, which is lower than the validation data trained over 8 epochs. This appears to be the model overfitting on the data from the training set, and so training it for a fewer number of epochs limits the overfitting. Documents accuracy15.png, accuracy8.png, loss15.png, and loss8.png show the accuracy and loss of the training data juxtaposed with the validation data for 15 epochs and 8 epochs.

There are a couple of aspects I hope to improve to my model. First, although the accuracy for the validation testing is relatively high, its loss does not fall, and in fact increases, as the number of epochs increase. It makes sense that for training for around 15 epochs, the loss increases since the model overfits. However, is unclear why the loss doesn't decrease at all. Trying more models and including more data hopefully might address that problem. Furthermore, because this model uses data from before the pandemic, it would be interesting to see how well this model identifies misleading news articles from before the pandemic. I could also train this model on titles on the pandemic to see how well the model would adapt.
